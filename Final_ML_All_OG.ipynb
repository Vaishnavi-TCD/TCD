{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPHFRr9cGi3HBLdvFmzoiLE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaishnavi-TCD/TCD/blob/main/Final_ML_All_OG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwk1xC1ONn8d",
        "outputId": "45f41c8f-3898-450b-d4e7-108219a14fa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.651214 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-fb5a7b478578>:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.exp(torch.tensor(loss))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 2.6910, val loss 2.6946, train perplexity 14.7471, val perplexity 14.7997, train accuracy 0.0665, val accuracy 0.0701\n",
            "step 500: train loss 1.6156, val loss 1.5390, train perplexity 5.0308, val perplexity 4.6599, train accuracy 0.4043, val accuracy 0.4275\n",
            "step 1000: train loss 1.3920, val loss 1.3461, train perplexity 4.0230, val perplexity 3.8426, train accuracy 0.5115, val accuracy 0.5256\n",
            "step 1500: train loss 1.3045, val loss 1.2728, train perplexity 3.6858, val perplexity 3.5708, train accuracy 0.5485, val accuracy 0.5591\n",
            "step 2000: train loss 1.2738, val loss 1.2534, train perplexity 3.5743, val perplexity 3.5022, train accuracy 0.5599, val accuracy 0.5659\n",
            "step 2500: train loss 1.2471, val loss 1.2409, train perplexity 3.4803, val perplexity 3.4588, train accuracy 0.5695, val accuracy 0.5726\n",
            "step 3000: train loss 1.2364, val loss 1.2377, train perplexity 3.4434, val perplexity 3.4475, train accuracy 0.5738, val accuracy 0.5752\n",
            "step 3500: train loss 1.2251, val loss 1.2275, train perplexity 3.4046, val perplexity 3.4126, train accuracy 0.5777, val accuracy 0.5777\n",
            "step 4000: train loss 1.2065, val loss 1.2211, train perplexity 3.3418, val perplexity 3.3911, train accuracy 0.5844, val accuracy 0.5811\n",
            "step 4500: train loss 1.2004, val loss 1.2238, train perplexity 3.3216, val perplexity 3.4000, train accuracy 0.5873, val accuracy 0.5810\n",
            "step 4999: train loss 1.1870, val loss 1.2185, train perplexity 3.2772, val perplexity 3.3820, train accuracy 0.5923, val accuracy 0.5837\n",
            "Generated Melody (GPT Model):\n",
            "\n",
            "RfEEDFFEECEDCBCCCCCCEECCDCCARAffEfAfEDRfffREEDFFEEDRFDaFEDDFEECCGCCCARAAGGfRCCCCCDEFFEEDCgFRAAacFECECGCCCCCCEGCGACRGFCCAaARAAAAGGfRDaDCCCCDEFFECCEGCECEDEFECCECREEDCEFFECCECCCGCGACRACCCARGFGAaccccaEFCEEDDCEFECCGFEECGFEEDDEFCCEECCCCCDDDRBCCCCCaCCCEGCECCCCCCCCCARDCCARAAGGDCCRCCCCCCARFFcEEEDEFECCEFECREFECCgEECCCCCaaCCCGACRAGGRFCCFfEEDDEEFECCCCGFGECCCCGFEEEDCCECCCCCCCCDDARFFcBBBBBcDBDCEFECCCGFEECCCCCCDRFFcECCCGFGECGCEFECCCGFEECCCCCGFEECCCARAGGRFFcECCCCdCCGFCEECCCGFECCCCGGFECCCCCCGFEEECCCGFEECCCCCGFEE\n",
            "BLEU Score: 0.1324\n",
            "Generated Melody (Unigram Model):\n",
            "EBRAEdfFGRBBDEffgARaBADafGdCRGDgaRgFRdfBFCggFBcffFRGACaEggFFEAgfaDARfRGCEAfEAaABBgGdRGdagDfDaBffBafDARRDFFaAFcDGACCFEdaDFcaEcfgDDcgBEDBccgDRdCDRGAAacafdGDcCRcBdFA\n",
            "aFBBaaFDgCEBFdCcDFCaFgcdAgcDRBCRaEBGBCdEGfgGCEBRDaACDaFfAFFDaGBFgDDfcddGcfFfCdRdRGECcaREFDAffGgcAdBgBEdRfFgfEARBAFagCGCBGgaBdACafBBagcEFaFDFBEdGCaFEBfCBaaBRagcRdBAEBDBaEBRggEDEAgfGDFaDBdcdAFAgFAGDcDCAcFRcfGARcgAGfRGACDCAFRaCCFffRECBRGCRcacdARDGFfCcFafRBAgBEBDBCfFFGBGAEcBCCERfdDEdFaRBECdEfAgGBaggCdgcFfdcdfAFDFdRCGggDECddGFREGgGfdaCEAFCD\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from collections import Counter\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import random\n",
        "\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "dropout = 0.2\n",
        "\n",
        "n_embd= 256\n",
        "n_head = 2\n",
        "n_layer = 2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('inputMelodiesAugmented.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "text = text.replace(' ', '')\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        accuracies = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "            # Reshape logits to match (B, T, C)\n",
        "            B, T = X.shape\n",
        "            logits = logits.view(B, T, -1)  # Restore original shape\n",
        "            predictions = logits.argmax(dim=-1)  # Get predicted indices\n",
        "\n",
        "            # Calculate accuracy\n",
        "            accuracies[k] = (predictions == Y).float().mean().item()  # Compare predictions with targets\n",
        "\n",
        "        out[split] = {\n",
        "            'loss': losses.mean(),\n",
        "            'accuracy': accuracies.mean()\n",
        "        }\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_perplexity(loss):\n",
        "    return torch.exp(torch.tensor(loss))\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_bleu(reference, candidate):\n",
        "    # BLEU score computation\n",
        "    reference = [list(reference)]  # BLEU expects a list of reference sequences\n",
        "    candidate = list(candidate)\n",
        "    return sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "# Baseline unigram model\n",
        "def unigram_model(data):\n",
        "    freq = Counter(data.tolist())\n",
        "    total = sum(freq.values())\n",
        "    prob = {k: v / total for k, v in freq.items()}\n",
        "    return prob\n",
        "\n",
        "unigram_probs = unigram_model(train_data)\n",
        "\n",
        "def unigram_generate(length):\n",
        "    return ''.join([itos[random.choices(list(unigram_probs.keys()), list(unigram_probs.values()))[0]] for _ in range(length)])\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss, accuracy, and BLEU score\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        metrics = estimate_loss()\n",
        "        perplexity = {split: calculate_perplexity(metrics[split]['loss']) for split in ['train', 'val']}\n",
        "        print(f\"step {iter}: train loss {metrics['train']['loss']:.4f}, val loss {metrics['val']['loss']:.4f}, train perplexity {perplexity['train']:.4f}, val perplexity {perplexity['val']:.4f}, train accuracy {metrics['train']['accuracy']:.4f}, val accuracy {metrics['val']['accuracy']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_melody = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(\"Generated Melody (GPT Model):\")\n",
        "print(generated_melody)\n",
        "\n",
        "# Calculate BLEU score against a reference (for demonstration purposes, use a subset of the dataset as reference)\n",
        "reference_melody = decode(val_data[:500].tolist())\n",
        "bleu_score = calculate_bleu(reference_melody, generated_melody)\n",
        "print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "# Generate melody using unigram baseline\n",
        "print(\"Generated Melody (Unigram Model):\")\n",
        "unigram_generated = unigram_generate(500)\n",
        "print(unigram_generated)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation Steps\n",
        "# normalize the dataset\n",
        "# Normalization: Convert all notes to uppercase\n",
        "def normalize_notes(melodies):\n",
        "    normalized = []\n",
        "    for melody in melodies:\n",
        "        normalized.append(melody.strip().upper())  # Convert to uppercase and remove extra spaces\n",
        "    return normalized\n",
        "\n",
        "# Load melodies\n",
        "with open('inputMelodiesAugmented.txt', 'r') as file:\n",
        "    melodies = file.readlines()\n",
        "\n",
        "# Apply normalization\n",
        "normalized_melodies = normalize_notes(melodies)\n",
        "\n",
        "# Save normalized melodies\n",
        "with open('normalizedMelodies.txt', 'w') as file:\n",
        "    for melody in normalized_melodies:\n",
        "        file.write(melody + '\\n')\n",
        "\n",
        "print(\"Normalization completed. Saved to 'normalizedMelodies.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAf4lJw5OsYy",
        "outputId": "7ba9daf8-b39a-4640-acab-2f16a80c41e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalization completed. Saved to 'normalizedMelodies.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# introduce rhythm and duration\n",
        "import random\n",
        "\n",
        "# Define rhythmic durations\n",
        "durations = [\"1/4\", \"1/8\", \"1/16\"]\n",
        "\n",
        "# Add rhythm and duration to notes\n",
        "def add_rhythm(melodies):\n",
        "    rhythmic_melodies = []\n",
        "    for melody in melodies:\n",
        "        rhythmic_melody = []\n",
        "        for note in melody.split():\n",
        "            if note != \"R\":  # Only notes get durations, rests remain the same\n",
        "                rhythmic_melody.append(f\"{note}:{random.choice(durations)}\")\n",
        "            else:\n",
        "                rhythmic_melody.append(note)  # Keep rests as is\n",
        "        rhythmic_melodies.append(\" \".join(rhythmic_melody))\n",
        "    return rhythmic_melodies\n",
        "\n",
        "# Apply rhythm\n",
        "rhythmic_melodies = add_rhythm(normalized_melodies)\n",
        "\n",
        "# Save rhythmic melodies\n",
        "with open('rhythmicMelodies.txt', 'w') as file:\n",
        "    for melody in rhythmic_melodies:\n",
        "        file.write(melody + '\\n')\n",
        "\n",
        "print(\"Rhythm added. Saved to 'rhythmicMelodies.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75xIhLVvRgZa",
        "outputId": "b728811b-d29b-45c2-f0df-13a1691269ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rhythm added. Saved to 'rhythmicMelodies.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ocatve expansion.. transpose melodies into lower and higher ocatves\n",
        "# Define pitch-shifting function\n",
        "def transpose_octave(melodies, shift):\n",
        "    notes = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\", \"R\"]\n",
        "    transposed_melodies = []\n",
        "    for melody in melodies:\n",
        "        transposed_melody = []\n",
        "        for token in melody.split():\n",
        "            if \":\" in token:  # Token with duration\n",
        "                note, duration = token.split(\":\")\n",
        "                if note in notes and note != \"R\":\n",
        "                    index = notes.index(note)\n",
        "                    transposed_note = notes[(index + shift) % 12]  # Transpose note\n",
        "                    transposed_melody.append(f\"{transposed_note}:{duration}\")\n",
        "                else:\n",
        "                    transposed_melody.append(token)  # Keep rests unchanged\n",
        "            elif token in notes and token != \"R\":  # Plain note\n",
        "                index = notes.index(token)\n",
        "                transposed_note = notes[(index + shift) % 12]\n",
        "                transposed_melody.append(transposed_note)\n",
        "            else:\n",
        "                transposed_melody.append(token)\n",
        "        transposed_melodies.append(\" \".join(transposed_melody))\n",
        "    return transposed_melodies\n",
        "\n",
        "# Apply octave expansion\n",
        "expanded_melodies = []\n",
        "for shift in [-1, 0, 1]:  # Shift down an octave, keep original, shift up an octave\n",
        "    expanded_melodies += transpose_octave(rhythmic_melodies, shift)\n",
        "\n",
        "# Save expanded melodies\n",
        "with open('expandedMelodies.txt', 'w') as file:\n",
        "    for melody in expanded_melodies:\n",
        "        file.write(melody + '\\n')\n",
        "\n",
        "print(\"Octave expansion completed. Saved to 'expandedMelodies.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmwBoeJ8Rikr",
        "outputId": "2f9e68c1-57a3-4ce6-dd85-f93e61a87a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Octave expansion completed. Saved to 'expandedMelodies.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# additional data sugmentation techniques\n",
        "#Apply random pitch-shifting, inversion, and noise injection.\n",
        "# Data Augmentation Techniques\n",
        "\n",
        "# Random pitch shifting\n",
        "def random_pitch_shift(melodies, semitones_range=2):\n",
        "    notes = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\", \"R\"]\n",
        "    shifted_melodies = []\n",
        "    for melody in melodies:\n",
        "        shifted_melody = []\n",
        "        shift = random.randint(-semitones_range, semitones_range)  # Random shift within range\n",
        "        for token in melody.split():\n",
        "            if \":\" in token:\n",
        "                note, duration = token.split(\":\")\n",
        "                if note in notes and note != \"R\":\n",
        "                    index = notes.index(note)\n",
        "                    shifted_note = notes[(index + shift) % 12]\n",
        "                    shifted_melody.append(f\"{shifted_note}:{duration}\")\n",
        "                else:\n",
        "                    shifted_melody.append(token)\n",
        "            elif token in notes and token != \"R\":\n",
        "                index = notes.index(token)\n",
        "                shifted_note = notes[(index + shift) % 12]\n",
        "                shifted_melody.append(shifted_note)\n",
        "            else:\n",
        "                shifted_melody.append(token)\n",
        "        shifted_melodies.append(\" \".join(shifted_melody))\n",
        "    return shifted_melodies\n",
        "\n",
        "# Invert melody\n",
        "def invert_melody(melodies):\n",
        "    notes = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\", \"R\"]\n",
        "    inverted_melodies = []\n",
        "    for melody in melodies:\n",
        "        inverted_melody = []\n",
        "        for token in melody.split():\n",
        "            if \":\" in token:\n",
        "                note, duration = token.split(\":\")\n",
        "                if note in notes and note != \"R\":\n",
        "                    index = notes.index(note)\n",
        "                    inverted_note = notes[-(index + 1)]  # Invert by reversing the index\n",
        "                    inverted_melody.append(f\"{inverted_note}:{duration}\")\n",
        "                else:\n",
        "                    inverted_melody.append(token)\n",
        "            else:\n",
        "                inverted_melody.append(token)\n",
        "        inverted_melodies.append(\" \".join(inverted_melody))\n",
        "    return inverted_melodies\n",
        "\n",
        "# Apply augmentations\n",
        "pitch_shifted_melodies = random_pitch_shift(expanded_melodies)\n",
        "inverted_melodies = invert_melody(expanded_melodies)\n",
        "\n",
        "# Combine all augmented datasets\n",
        "augmented_dataset = expanded_melodies + pitch_shifted_melodies + inverted_melodies\n",
        "\n",
        "# Save the final augmented dataset\n",
        "with open('finalAugmentedMelodies.txt', 'w') as file:\n",
        "    for melody in augmented_dataset:\n",
        "        file.write(melody + '\\n')\n",
        "\n",
        "print(\"Data augmentation completed. Saved to 'finalAugmentedMelodies.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOSpGQY1RlGZ",
        "outputId": "0287a733-93e0-45b8-ed33-51331205472f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data augmentation completed. Saved to 'finalAugmentedMelodies.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from collections import Counter\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import random\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "dropout = 0.2\n",
        "\n",
        "n_embd= 256\n",
        "n_head = 2\n",
        "n_layer = 2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Load melody data\n",
        "with open('/content/finalAugmentedMelodies.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "text = text.replace(' ', '')  # Remove spaces between tokens\n",
        "\n",
        "# Define the vocabulary for musical notes\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from notes/rests to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        accuracies = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "            # Reshape logits to match (B, T, C)\n",
        "            B, T = X.shape\n",
        "            logits = logits.view(B, T, -1)  # Restore original shape\n",
        "            predictions = logits.argmax(dim=-1)  # Get predicted indices\n",
        "\n",
        "            # Calculate accuracy\n",
        "            accuracies[k] = (predictions == Y).float().mean().item()  # Compare predictions with targets\n",
        "\n",
        "        out[split] = {\n",
        "            'loss': losses.mean(),\n",
        "            'accuracy': accuracies.mean()\n",
        "        }\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_perplexity(loss):\n",
        "    return torch.exp(torch.tensor(loss))\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_bleu(reference, candidate):\n",
        "    # BLEU score computation\n",
        "    reference = [list(reference)]  # BLEU expects a list of reference sequences\n",
        "    candidate = list(candidate)\n",
        "    return sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "# Baseline unigram model\n",
        "def unigram_model(data):\n",
        "    freq = Counter(data.tolist())\n",
        "    total = sum(freq.values())\n",
        "    prob = {k: v / total for k, v in freq.items()}\n",
        "    return prob\n",
        "\n",
        "unigram_probs = unigram_model(train_data)\n",
        "\n",
        "def unigram_generate(length):\n",
        "    return ''.join([itos[random.choices(list(unigram_probs.keys()), list(unigram_probs.values()))[0]] for _ in range(length)])\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss, accuracy, and BLEU score\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        metrics = estimate_loss()\n",
        "        perplexity = {split: calculate_perplexity(metrics[split]['loss']) for split in ['train', 'val']}\n",
        "        print(f\"step {iter}: train loss {metrics['train']['loss']:.4f}, val loss {metrics['val']['loss']:.4f}, train perplexity {perplexity['train']:.4f}, val perplexity {perplexity['val']:.4f}, train accuracy {metrics['train']['accuracy']:.4f}, val accuracy {metrics['val']['accuracy']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_melody = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(\"Generated Melody (GPT Model):\")\n",
        "print(generated_melody)\n",
        "\n",
        "# Calculate BLEU score against a reference (for demonstration purposes, use a subset of the dataset as reference)\n",
        "reference_melody = decode(val_data[:500].tolist())\n",
        "bleu_score = calculate_bleu(reference_melody, generated_melody)\n",
        "print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "# Calculate BLEU score for the unigram model\n",
        "unigram_generated = unigram_generate(500)\n",
        "unigram_bleu_score = calculate_bleu(reference_melody, unigram_generated)\n",
        "print(f\"BLEU Score (Unigram Model): {unigram_bleu_score:.4f}\")\n",
        "\n",
        "\n",
        "# Generate melody using unigram baseline\n",
        "print(\"Generated Melody (Unigram Model):\")\n",
        "print(unigram_generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRx6MtLsRoVe",
        "outputId": "b5c8b70a-e907-43f3-89af-4251a4df1d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.651727 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-ed86fc7054ed>:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.exp(torch.tensor(loss))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 2.8170, val loss 2.8147, train perplexity 16.7271, val perplexity 16.6874, train accuracy 0.1070, val accuracy 0.1088\n",
            "step 500: train loss 1.4998, val loss 1.4944, train perplexity 4.4810, val perplexity 4.4567, train accuracy 0.4227, val accuracy 0.4263\n",
            "step 1000: train loss 1.2736, val loss 1.2690, train perplexity 3.5736, val perplexity 3.5571, train accuracy 0.5393, val accuracy 0.5407\n",
            "step 1500: train loss 1.2077, val loss 1.2045, train perplexity 3.3457, val perplexity 3.3352, train accuracy 0.5665, val accuracy 0.5683\n",
            "step 2000: train loss 1.1921, val loss 1.1876, train perplexity 3.2941, val perplexity 3.2791, train accuracy 0.5716, val accuracy 0.5745\n",
            "step 2500: train loss 1.1730, val loss 1.1681, train perplexity 3.2315, val perplexity 3.2158, train accuracy 0.5802, val accuracy 0.5825\n",
            "step 3000: train loss 1.1740, val loss 1.1746, train perplexity 3.2350, val perplexity 3.2370, train accuracy 0.5791, val accuracy 0.5794\n",
            "step 3500: train loss 1.1586, val loss 1.1540, train perplexity 3.1854, val perplexity 3.1707, train accuracy 0.5881, val accuracy 0.5899\n",
            "step 4000: train loss 1.1395, val loss 1.1371, train perplexity 3.1252, val perplexity 3.1178, train accuracy 0.5943, val accuracy 0.5956\n",
            "step 4500: train loss 1.1273, val loss 1.1302, train perplexity 3.0873, val perplexity 3.0963, train accuracy 0.6004, val accuracy 0.5997\n",
            "step 4999: train loss 1.1196, val loss 1.1194, train perplexity 3.0635, val perplexity 3.0631, train accuracy 0.6029, val accuracy 0.6027\n",
            "Generated Melody (GPT Model):\n",
            "\n",
            "RFEGGAGARFEDCRDGGEDCRGGGEDCRCEFFGEDCDCDFCRCCCDCBEDDGERCEFGAAAAARAAAGABGFERFFERFERFE:1/16\n",
            "GGGGGGDFGGGFGGGGGDFGGFGGGGAFDDCCFFGGGGGGGGDCCFFFFFGGFFDFGCCRGGGAFERGGGGGDFGGGGGFGDFGGFFGGGGGGGGGFGGGGDFDCDCFFGDFGGGGGGGGAFDDCAFGGGGGGGGGGGGGGDFGFFDDCDCFCCRGGGAFRGGGFGGGGAFFDFDDCDCFFGGGGGGGGGFFFDDCDCDCFFGFFFDDCCFGDCGGGGGGGGGDFGGGGGGGGGGGGFGFGGGGGDFDFDDDCDCFFFFFGGGGGGGGGGGGGGGGGGDGAGGGGGGGGGGGGGGGGGGGGFGGGGGDFDCDCFGGGGGGDFDCFGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGFGFGGGGGGGGGGGGGGGGFGGGGGGGGGGGGGGGGGGGGFGGGGGGGFGGRGGFG\n",
            "BLEU Score: 0.2730\n",
            "BLEU Score (Unigram Model): 0.2488\n",
            "Generated Melody (Unigram Model):\n",
            "AAGEAFFFAFARDDCRCGGDADEFBDFGRC/RGFBCBFEFGGGEEDCARDBCGCCEGDRBFDBGAFBAABCCABAFFADCEDGBFGDDGAAFGBRDGFREDFRFCGGEGRGDADGFAEEDFFARCFRGCFBDCARDFAEFRCABGERAADFCACRRFFEEGRGCDAGGBARAADECFRAGBFA\n",
            "CCFCRCADDCCCBRGDBGBGFCDFFFRCGDRCDGACFCEGFRBFCFEB8BCBDRDGFFDAAACEGBCCEAEGADBDFEFDGGCGFBRGDFDCDDFRCGGAEGERAFFBFFRC/AGCFDGACADCAGACGRGGDFEECDBABBGDFCDGG/ACAAGDDGBFGDEGBAGBABCCAADGFDFCF1GDACBAGAACADEAABDEFGERFGBFRBRDERGRGGFGFABCGBDRDGGDRDGERCADGRFFFCEDRDFGCBGGGBDGCGEAGCDCBFFDADCRBCGECGGACGDBFGAGFCABAGCCEGACCEEGAFDFGD1F\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from collections import Counter\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import random\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "dropout = 0.3\n",
        "\n",
        "n_embd = 512\n",
        "n_head = 8\n",
        "n_layer = 6\n",
        "# Gradient clipping threshold\n",
        "grad_clip = 1.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Load melody data\n",
        "with open('/content/finalAugmentedMelodies.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "text = text.replace(' ', '')  # Remove spaces between tokens\n",
        "\n",
        "# Define the vocabulary for musical notes\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from notes/rests to integers\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        accuracies = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "            # Reshape logits to match (B, T, C)\n",
        "            B, T = X.shape\n",
        "            logits = logits.view(B, T, -1)  # Restore original shape\n",
        "            predictions = logits.argmax(dim=-1)  # Get predicted indices\n",
        "\n",
        "            # Calculate accuracy\n",
        "            accuracies[k] = (predictions == Y).float().mean().item()  # Compare predictions with targets\n",
        "\n",
        "        out[split] = {\n",
        "            'loss': losses.mean(),\n",
        "            'accuracy': accuracies.mean()\n",
        "        }\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_perplexity(loss):\n",
        "    return torch.exp(torch.tensor(loss))\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_bleu(reference, candidate):\n",
        "    # BLEU score computation\n",
        "    reference = [list(reference)]  # BLEU expects a list of reference sequences\n",
        "    candidate = list(candidate)\n",
        "    return sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "# Baseline unigram model\n",
        "def unigram_model(data):\n",
        "    freq = Counter(data.tolist())\n",
        "    total = sum(freq.values())\n",
        "    prob = {k: v / total for k, v in freq.items()}\n",
        "    return prob\n",
        "\n",
        "unigram_probs = unigram_model(train_data)\n",
        "\n",
        "def unigram_generate(length):\n",
        "    return ''.join([itos[random.choices(list(unigram_probs.keys()), list(unigram_probs.values()))[0]] for _ in range(length)])\n",
        "\n",
        "class RelativePositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.encoding = nn.Parameter(torch.zeros(max_len, d_model))\n",
        "\n",
        "        # Initialize positional encodings\n",
        "        nn.init.xavier_uniform_(self.encoding)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape  # Batch size and sequence length\n",
        "        if T > self.max_len:\n",
        "            raise ValueError(\"Sequence length exceeds the maximum length of positional encoding.\")\n",
        "        return self.encoding[:T, :]  # Return encodings for the given sequence length\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B,T,hs)\n",
        "        q = self.query(x)  # (B,T,hs)\n",
        "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5  # Scaled dot-product attention\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.relative_pos_encoding = RelativePositionalEncoding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.relative_pos_encoding(idx)  # Adjusted for sequence length\n",
        "        pos_emb = pos_emb.unsqueeze(0).expand(B, -1, -1)  # Match batch size\n",
        "        x = tok_emb + pos_emb  # Combine embeddings\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = logits.view(B * T, -1)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5, verbose=True)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        metrics = estimate_loss()\n",
        "        perplexity = {split: calculate_perplexity(metrics[split]['loss']) for split in ['train', 'val']}\n",
        "        print(f\"step {iter}: train loss {metrics['train']['loss']:.4f}, val loss {metrics['val']['loss']:.4f}, train perplexity {perplexity['train']:.4f}, val perplexity {perplexity['val']:.4f}, train accuracy {metrics['train']['accuracy']:.4f}, val accuracy {metrics['val']['accuracy']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Function to save generated tokens to a text file\n",
        "def save_tokens_to_file(tokens, filename='generated_tokens.txt'):\n",
        "    \"\"\"\n",
        "    Saves the generated tokens to a text file.\n",
        "    \"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        for token in tokens:\n",
        "            f.write(f\"{itos[token]} \")  # Write each token as its corresponding character\n",
        "    print(f\"Saved generated tokens to {filename}\")\n",
        "\n",
        "# Generate melody using the GPT model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_tokens = m.generate(context, max_new_tokens=500)[0].tolist()\n",
        "\n",
        "# Decode tokens to melody string\n",
        "generated_melody = decode(generated_tokens)\n",
        "print(\"Generated Melody (GPT Model):\")\n",
        "print(generated_melody)\n",
        "\n",
        "# Save the generated tokens to a text file\n",
        "save_tokens_to_file(generated_tokens)\n",
        "\n",
        "# Evaluate BLEU score\n",
        "reference_melody = decode(val_data[:500].tolist())\n",
        "bleu_score = calculate_bleu(reference_melody, generated_melody)\n",
        "print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "# Unigram model comparison\n",
        "unigram_generated = unigram_generate(500)\n",
        "unigram_bleu_score = calculate_bleu(reference_melody, unigram_generated)\n",
        "print(f\"BLEU Score (Unigram Model): {unigram_bleu_score:.4f}\")\n",
        "print(\"Generated Melody (Unigram Model):\")\n",
        "print(unigram_generated)\n",
        "\n"
      ],
      "metadata": {
        "id": "72wrnZbxw1FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mido\n",
        "from mido import MidiFile, MidiTrack, Message\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "from pydub.generators import Sine\n",
        "\n",
        "# Define a function to clean tokens\n",
        "def clean_tokens(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        tokens = f.read()\n",
        "    tokens = tokens.replace(':', '').replace('/', '').replace('1', '').replace('4', '')\n",
        "    tokens = ''.join(filter(lambda x: x.isalpha() or x == ' ', tokens))\n",
        "    return tokens.split()\n",
        "\n",
        "# Define a mapping of notes to frequencies (simplified)\n",
        "note_to_freq = {\n",
        "    'C': 261.63,  # Middle C\n",
        "    'D': 293.66,\n",
        "    'E': 329.63,\n",
        "    'F': 349.23,\n",
        "    'G': 392.00,\n",
        "    'A': 440.00,\n",
        "    'B': 493.88,\n",
        "    'R': 0.0  # Rest\n",
        "}\n",
        "\n",
        "# Function to generate MIDI from tokens\n",
        "def generate_midi(tokens, midi_file_path):\n",
        "    mid = MidiFile()\n",
        "    track = MidiTrack()\n",
        "    mid.tracks.append(track)\n",
        "\n",
        "    for token in tokens:\n",
        "        if token in note_to_freq:\n",
        "            if token == 'R':\n",
        "                track.append(Message('note_off', note=0, velocity=0, time=480))\n",
        "            else:\n",
        "                midi_note = int(69 + 12 * np.log2(note_to_freq[token] / 440.0))\n",
        "                track.append(Message('note_on', note=midi_note, velocity=64, time=0))\n",
        "                track.append(Message('note_off', note=midi_note, velocity=64, time=480))\n",
        "\n",
        "    mid.save(midi_file_path)\n",
        "\n",
        "\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    input_file = 'generated_tokens.txt'\n",
        "    midi_output = 'generated_melody.mid'\n",
        "\n",
        "    # input_file = 'inputMelodies.txt'\n",
        "    # midi_output = 'input_melody.mid'\n",
        "\n",
        "\n",
        "    print(\"Cleaning tokens...\")\n",
        "    tokens = clean_tokens(input_file)\n",
        "\n",
        "    print(\"Generating MIDI file...\")\n",
        "    generate_midi(tokens, midi_output)\n",
        "    print(f\"MIDI file saved as {midi_output}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAxWmcMS6IFc",
        "outputId": "f10dd12d-0e72-4e1b-fc2d-f2869f436edb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning tokens...\n",
            "Generating MIDI file...\n",
            "MIDI file saved as generated_melody.mid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Architectural Enhacements:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from collections import Counter\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import random\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "dropout = 0.3\n",
        "\n",
        "n_embd= 512\n",
        "n_head = 8\n",
        "n_layer = 6\n",
        "# Gradient clipping threshold\n",
        "grad_clip = 1.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Load melody data\n",
        "with open('/content/finalAugmentedMelodies.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "text = text.replace(' ', '')  # Remove spaces between tokens\n",
        "\n",
        "# Define the vocabulary for musical notes\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from notes/rests to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        accuracies = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "            # Reshape logits to match (B, T, C)\n",
        "            B, T = X.shape\n",
        "            logits = logits.view(B, T, -1)  # Restore original shape\n",
        "            predictions = logits.argmax(dim=-1)  # Get predicted indices\n",
        "\n",
        "            # Calculate accuracy\n",
        "            accuracies[k] = (predictions == Y).float().mean().item()  # Compare predictions with targets\n",
        "\n",
        "        out[split] = {\n",
        "            'loss': losses.mean(),\n",
        "            'accuracy': accuracies.mean()\n",
        "        }\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_perplexity(loss):\n",
        "    return torch.exp(torch.tensor(loss))\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_bleu(reference, candidate):\n",
        "    # BLEU score computation\n",
        "    reference = [list(reference)]  # BLEU expects a list of reference sequences\n",
        "    candidate = list(candidate)\n",
        "    return sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "# Baseline unigram model\n",
        "def unigram_model(data):\n",
        "    freq = Counter(data.tolist())\n",
        "    total = sum(freq.values())\n",
        "    prob = {k: v / total for k, v in freq.items()}\n",
        "    return prob\n",
        "\n",
        "unigram_probs = unigram_model(train_data)\n",
        "\n",
        "def unigram_generate(length):\n",
        "    return ''.join([itos[random.choices(list(unigram_probs.keys()), list(unigram_probs.values()))[0]] for _ in range(length)])\n",
        "\n",
        "class RelativePositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.encoding = nn.Parameter(torch.zeros(max_len, d_model))\n",
        "\n",
        "        # Initialize positional encodings\n",
        "        nn.init.xavier_uniform_(self.encoding)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape  # Batch size and sequence length\n",
        "        if T > self.max_len:\n",
        "            raise ValueError(\"Sequence length exceeds the maximum length of positional encoding.\")\n",
        "        return self.encoding[:T, :]  # Return encodings for the given sequence length\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # Scaled dot-product attention\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.relative_pos_encoding = RelativePositionalEncoding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.relative_pos_encoding(idx)  # Adjusted for sequence length\n",
        "        pos_emb = pos_emb.unsqueeze(0).expand(B, -1, -1)  # Match batch size\n",
        "        x = tok_emb + pos_emb  # Combine embeddings\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = logits.view(B * T, -1)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        metrics = estimate_loss()\n",
        "        perplexity = {split: calculate_perplexity(metrics[split]['loss']) for split in ['train', 'val']}\n",
        "        print(f\"step {iter}: train loss {metrics['train']['loss']:.4f}, val loss {metrics['val']['loss']:.4f}, train perplexity {perplexity['train']:.4f}, val perplexity {perplexity['val']:.4f}, train accuracy {metrics['train']['accuracy']:.4f}, val accuracy {metrics['val']['accuracy']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_melody = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(\"Generated Melody (GPT Model):\")\n",
        "print(generated_melody)\n",
        "\n",
        "reference_melody = decode(val_data[:500].tolist())\n",
        "bleu_score = calculate_bleu(reference_melody, generated_melody)\n",
        "print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "unigram_generated = unigram_generate(500)\n",
        "unigram_bleu_score = calculate_bleu(reference_melody, unigram_generated)\n",
        "print(f\"BLEU Score (Unigram Model): {unigram_bleu_score:.4f}\")\n",
        "\n",
        "print(\"Generated Melody (Unigram Model):\")\n",
        "print(unigram_generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02r7oZZzZODf",
        "outputId": "93032a8d-45ba-4fde-ddb1-e73d575ea79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19.052559 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-366085a6911e>:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.exp(torch.tensor(loss))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 2.8724, val loss 2.8751, train perplexity 17.6788, val perplexity 17.7272, train accuracy 0.0655, val accuracy 0.0650\n",
            "step 500: train loss 1.5286, val loss 1.5196, train perplexity 4.6119, val perplexity 4.5704, train accuracy 0.4112, val accuracy 0.4131\n",
            "step 1000: train loss 1.1970, val loss 1.1895, train perplexity 3.3103, val perplexity 3.2855, train accuracy 0.5689, val accuracy 0.5720\n",
            "step 1500: train loss 1.0662, val loss 1.0633, train perplexity 2.9043, val perplexity 2.8960, train accuracy 0.6216, val accuracy 0.6225\n",
            "step 2000: train loss 1.0209, val loss 1.0132, train perplexity 2.7756, val perplexity 2.7543, train accuracy 0.6374, val accuracy 0.6403\n",
            "step 2500: train loss 0.9652, val loss 0.9629, train perplexity 2.6252, val perplexity 2.6193, train accuracy 0.6549, val accuracy 0.6559\n",
            "step 2999: train loss 0.9012, val loss 0.9006, train perplexity 2.4625, val perplexity 2.4610, train accuracy 0.6742, val accuracy 0.6748\n",
            "Generated Melody (GPT Model):\n",
            "\n",
            "REFGFFFFFGFFEREFEGGFGGFGAGFGFGFRFGFFFFREFEGGGFGGFRFGFFFREFECBGERFFFFFFFGFGGFGFGFGFFREFCBERFECBGERFFGFFFFGFFFREFEGGFGGGFFGFREFEGGFGFGFREFEGFGGFGFFFREFECBBERFERFECBGEDRAGFFFFFGFEEDEDABAGFFFAGGFFDABAAGABGGFGFEGCBCEGBGGERFFECABEDDCCDCDCDCADCCGBCEBBADGGFRAGBABAECBABBBBBCBADGFFADDCRBCDFDCBCDEGGRBDCDEGGRBDCDERDCDEGGRBDCDEGGRBDCDEGGRBDCDERFGGRBBCBCBCBGGABCEBRAGGBBE:1/4\n",
            "RCCGCGCGGACADAAAGRCCGCGCGGAGCGCCCCGGCGCCGCGGAGFGCCCCGCGCGGGDDCACCCDCCCGAGAGDDCAGACCDEDCCCDCCRCCDEDCCCDERCCDEDCARADDCCCCCDCAGFGFGFGFGFGFA\n",
            "BLEU Score: 0.2637\n",
            "BLEU Score (Unigram Model): 0.2496\n",
            "Generated Melody (Unigram Model):\n",
            "CCGRDEGGEGRFGAEFFCABFDCEDACAGAAGFFFDRAARGFDFCEFEADB:DBDBAGGCCGGDGCADAEBGACAAAGBCAERDRCGEGCRCCDCCCEARBDEEGGBD1DCDRCAERCGCCFBGFDGDFFAADRACAFEGRCAEEFDCAGBGRBAADGFEFCGDGGRDGEFAFFFACADRGCCFAFGRAGRGAFGFRDFACFCAAEFDDAFCGGCGCABCGEAG/DCRADRCRARCDFFACDAGEEB4BDFCRAED1FCDGBRDEDFBFAERBAEGGFDBDBFAFACRGGAGGCBFGFGARGDCCEFBFFGBGCDDFABDBAEBCDDFGGERADBGGAAEAFGGBARBBGDBGAGFCCACFGGGFGDGDGBBAECCBAAACFFGF1ARGFECFGCACGEEBBCEAGGAEGEEGEFBBDFCECEDFFFGAAFCEAREDFAECDDFACCFRECGFCCCAFACFFDCEBCCBCCFFDRFAFBGFDCCAGCEACCFRGFCCDCD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qS-x-DUr_cA_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}